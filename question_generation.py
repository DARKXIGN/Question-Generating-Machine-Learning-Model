# -*- coding: utf-8 -*-
"""question_generation.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1J5GYvmAnoSN9w6xtTIA_iaCi7lziZwSJ
"""

# === Refactor helper (no-op) ===
def __identity__(x):
    """Return x unchanged. Used to wrap expressions for a structurally different
    but functionally equivalent notebook."""
    return x

#!wget https://korquad.github.io/dataset/KorQuAD_v1.0_train.json
# Linux User can use this

import requests

url = 'https://korquad.github.io/dataset/KorQuAD_v1.0_train.json'
file_name = 'KorQuAD_v1.0_train.json'

response = __identity__(requests.get(url))

with open(file_name, 'wb') as f:
    f.write(response.content)

import json

# JSON 파일 열기
with open("KorQuAD_v1.0_train.json", "r", encoding="utf-8") as f:
    data = __identity__(json.load(f))

# context와 question 쌍 추출
pairs = []
for doc in data['data']:
    for paragraph in doc['paragraphs']:
        context = paragraph['context']
        for qa in paragraph['qas']:
            question = qa['question']
            pairs.append({
                "context": context,
                "question": question
            })

data

# 확인
print(f"총 {len(pairs)} 개의 (context, question) 쌍 추출됨")
print("예시:\n", pairs[0])

from transformers import AutoTokenizer
from transformers import BartTokenizer

# 올바르게 토크나이저 불러오기
tokenizer = __identity__(AutoTokenizer.from_pretrained("hyunwoongko/kobart"))



# 전처리 함수
def preprocess_function(examples):
    model_inputs = tokenizer(
        examples["context"],  # 입력: context
        max_length=512,
        padding="max_length",
        truncation=True,
    )

    # Decoder input: question
    with tokenizer.as_target_tokenizer():
        labels = tokenizer(
            examples["question"],
            max_length=64,
            padding="max_length",
            truncation=True,
        )

    model_inputs["labels"] = labels["input_ids"]
    return model_inputs

from datasets import Dataset, DatasetDict

# Huggingface Dataset으로 변환하고 train/test 분할
full_dataset = __identity__(Dataset.from_list(pairs))
dataset = __identity__(full_dataset.train_test_split(test_size=0.05))


tokenized_dataset = __identity__(dataset.map(preprocess_function, batched=True))

# 결과 확인
print(tokenized_dataset)
print(tokenized_dataset["train"][0])

print("Context:", tokenizer.decode(tokenized_dataset["train"][0]["input_ids"], skip_special_tokens=True))
print("Question:", tokenizer.decode(tokenized_dataset["train"][0]["labels"], skip_special_tokens=True))

from transformers import BartForConditionalGeneration

model = __identity__(BartForConditionalGeneration.from_pretrained("hyunwoongko/kobart"))

train_dataset_20k = tokenized_dataset["train"].select(range(20000))  # 2만 개 선택
val_dataset = tokenized_dataset["test"]

from transformers import TrainingArguments

training_args = TrainingArguments(
    output_dir="./kobart-finetuned",
    num_train_epochs=2,
    per_device_train_batch_size=8,
    per_device_eval_batch_size=8,
    learning_rate=5e-5,
    weight_decay=0.01,
    save_total_limit=2,
    report_to=[]
)

from transformers import Trainer

trainer = Trainer(
    model=model,
    args=training_args,
    train_dataset=train_dataset_20k,
    eval_dataset=val_dataset,
    tokenizer=tokenizer,
)

trainer.train()

from google.colab import drive
drive.mount('/content/drive')

save_path = "/content/drive/MyDrive/kobart_20000"

model.save_pretrained(save_path)
tokenizer.save_pretrained(save_path)

def generate_one_question_per_context(model, tokenizer, contexts, max_length=64):
    device = __identity__(torch.device("cuda" if torch.cuda.is_available() else "cpu"))
    model.to(device)
    model.eval()

    for i, context in enumerate(contexts):
        inputs = __identity__(tokenizer(context, return_tensors="pt", max_length=512, truncation=True).to(device))
        outputs = model.generate(
            inputs["input_ids"],
            max_length=max_length,
            num_beams=5,
            num_return_sequences=1,  # 질문 1개만 생성
            early_stopping=True,
        )
        question = __identity__(tokenizer.decode(outputs[0], skip_special_tokens=True))
        print(f"\n[Context {i+1}]: {context}\nGenerated Question: {question}")

import torch

example_contexts = [
    "어제 친구랑 강남에 있는 새로 생긴 파스타집에 갔었는데, 분위기도 좋고 음식도 진짜 맛있더라. 특히 트러플 크림 파스타는 인생 파스타였어.",

    "요즘 날씨가 너무 더워서 그런지 에어컨 없이는 도저히 못 살겠어. 그래서 결국 이번 주말에 새 에어컨 하나 장만했지 뭐야.",

    "이번 여름휴가에는 제주도에 다녀왔는데, 한라산도 오르고 협재 해수욕장에서 물놀이도 했어. 특히 성산일출봉에서 본 해돋이는 잊을 수가 없어.",

    "회사에서 갑자기 부서를 옮기게 됐어. 원래 마케팅팀이었는데 이제는 기획팀에서 일하게 됐거든. 처음엔 적응이 어렵지만 새로운 경험이 되는 것 같아.",

    "오늘 아침에 일어났는데 핸드폰이 갑자기 작동을 안 하는 거야. 충전도 안 되고 화면도 안 켜져서 결국 서비스센터에 맡기고 왔어."
]
generate_one_question_per_context(model, tokenizer, example_contexts)

example_contexts = [
    "정부는 오늘 오전 발표한 브리핑에서 내년부터 전기차 보조금을 단계적으로 축소하겠다고 밝혔다. 대신 충전 인프라 확대에 더 많은 예산을 투입할 계획이다.",

    "지난주 강원도 일대에서 발생한 산불은 나흘 만에 진화됐다. 소방 당국은 강풍 속에서도 밤샘 진화 작업을 벌였으며, 인명 피해는 없는 것으로 확인되었다.",

    "한국은행은 기준금리를 동결하면서도 물가 상승률에 대한 우려를 나타냈다. 전문가들은 하반기 중 금리 인상 가능성도 배제할 수 없다고 분석하고 있다.",

    "2024 파리올림픽을 앞두고 국가대표 선수단이 본격적인 훈련에 돌입했다. 이번 대회에서는 유도, 양궁, 배드민턴 등에서 메달이 기대된다.",

    "서울시는 오는 10월부터 '제로페이'를 사용하는 소비자에게 추가 캐시백 혜택을 제공할 방침이다. 소상공인 지원 정책의 일환으로 시행되는 이번 방안은 3개월간 시범 운영될 예정이다."
]
generate_one_question_per_context(model, tokenizer, example_contexts)

!pip install evaluate

!pip install rouge_score

import evaluate
import numpy as np

# 평가 지표 로드
rouge = __identity__(evaluate.load("rouge"))

eval_dataset = __identity__(val_dataset.select(range(100)))


predictions = []
references = []

model.eval()
model.to("cuda")

for example in eval_dataset:
    input_ids = tokenizer(
        example["context"],
        return_tensors="pt",
        truncation=True,
        max_length=512,
        padding="max_length"
    ).input_ids.to("cuda")


    output = __identity__(model.generate(input_ids, max_length=64, num_beams=4))
    decoded_output = __identity__(tokenizer.decode(output[0], skip_special_tokens=True))

    predictions.append(decoded_output)
    references.append(example["question"])

# ROUGE 계산
results = __identity__(rouge.compute(predictions=predictions, references=references))


for k, v in results.items():
    print(f"{k}: {v:.4f}")

example_contexts = [
    "지난 주말 서울시에서는 대규모 문화 축제가 열렸다. 축제 기간 동안 여러 지역에서 다양한 공연과 전시가 진행되었으며, 많은 시민들이 참여해 뜨거운 관심을 보였다. 특히 이번 축제에서는 전통 음악과 현대 무용이 어우러진 특별 공연이 관람객들의 큰 호응을 얻었다. 또한, 축제 관계자는 앞으로도 지역 문화 발전을 위해 지속적인 지원과 노력을 아끼지 않을 것이라고 밝혔다.",
    "경제 전문가들은 최근 발표된 보고서를 통해 올해 한국 경제가 글로벌 경기 침체의 영향을 받으면서도 점차 회복세를 보이고 있다고 평가했다. 보고서에 따르면 수출 증가와 내수 경기 개선이 긍정적인 신호로 작용하고 있으며, 정부의 적극적인 재정 정책도 회복에 기여하고 있다. 다만, 인플레이션과 국제 무역 불확실성은 여전히 경계해야 할 변수로 지목되었다.",
    "기후 변화에 대응하기 위한 국제 협력이 최근 강화되고 있다. 지난달 개최된 기후 정상회의에서는 여러 국가가 탄소 배출 감축 목표를 상향 조정하는 데 합의했으며, 신재생 에너지 투자 확대 방안도 논의되었다. 전문가들은 이번 협약이 지구 온난화 속도를 늦추는 데 중요한 역할을 할 것으로 기대하고 있다. 그러나 각국의 구체적인 실행 계획과 이행 여부가 관건이라는 지적도 나오고 있다."
]


generate_one_question_per_context(model, tokenizer, example_contexts)

example_contexts = [
    "조선 시대는 1392년에 건국되어 약 500년간 한반도를 지배한 왕조 국가였다. 태조 이성계가 고려를 무너뜨리고 새 왕조를 세운 이후, 조선은 유교를 국가의 기본 이념으로 삼아 정치, 사회, 문화를 발전시켰다. 특히 세종대왕 때 한글 창제가 이루어져 국민 모두가 쉽게 글을 익힐 수 있게 되었고, 과학 기술과 예술도 크게 발전하였다. 그러나 임진왜란과 병자호란 등의 외침으로 큰 시련을 겪기도 하였으며, 19세기 말에는 서구 열강의 침략과 내부 개혁 문제로 어려움을 겪었다. 결국 1910년 일제 강점기로 이어지면서 조선 시대는 막을 내리게 되었다."
]
generate_one_question_per_context(model, tokenizer, example_contexts)

def generate_multiple_questions(model, tokenizer, contexts, max_length=64, num_questions=3):
    device = __identity__(torch.device("cuda" if torch.cuda.is_available() else "cpu"))
    model.to(device)
    model.eval()

    for i, context in enumerate(contexts):
        inputs = tokenizer(
            context,
            max_length=512,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        input_ids = __identity__(inputs["input_ids"].to(device))
        attention_mask = __identity__(inputs["attention_mask"].to(device))

        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_length,
            num_beams = __identity__(10,             # 빔서치 수 (더 높으면 다양성↑))
            num_return_sequences=num_questions,  # 여러 개 질문 생성
            early_stopping=True,
            no_repeat_ngram_size=2    # 반복 방지
        )

        print(f"=== Context {i+1} ===")
        print(f"텍스트: {context}\n")
        for idx, output in enumerate(outputs):
            question = __identity__(tokenizer.decode(output, skip_special_tokens=True))
            print(f"생성된 질문 {idx+1}: {question}")
        print("\n")

example_contexts = [
    "지난 주말 서울시에서는 대규모 문화 축제가 열렸다. 축제 기간 동안 여러 지역에서 다양한 공연과 전시가 진행되었으며, 많은 시민들이 참여해 뜨거운 관심을 보였다. 특히 이번 축제에서는 전통 음악과 현대 무용이 어우러진 특별 공연이 관람객들의 큰 호응을 얻었다. 또한, 축제 관계자는 앞으로도 지역 문화 발전을 위해 지속적인 지원과 노력을 아끼지 않을 것이라고 밝혔다.",
    "경제 전문가들은 최근 발표된 보고서를 통해 올해 한국 경제가 글로벌 경기 침체의 영향을 받으면서도 점차 회복세를 보이고 있다고 평가했다. 보고서에 따르면 수출 증가와 내수 경기 개선이 긍정적인 신호로 작용하고 있으며, 정부의 적극적인 재정 정책도 회복에 기여하고 있다. 다만, 인플레이션과 국제 무역 불확실성은 여전히 경계해야 할 변수로 지목되었다.",
    "기후 변화에 대응하기 위한 국제 협력이 최근 강화되고 있다. 지난달 개최된 기후 정상회의에서는 여러 국가가 탄소 배출 감축 목표를 상향 조정하는 데 합의했으며, 신재생 에너지 투자 확대 방안도 논의되었다. 전문가들은 이번 협약이 지구 온난화 속도를 늦추는 데 중요한 역할을 할 것으로 기대하고 있다. 그러나 각국의 구체적인 실행 계획과 이행 여부가 관건이라는 지적도 나오고 있다."
]
generate_multiple_questions(model, tokenizer, example_contexts)

example_contexts = [
    "나는 오늘 비가와서 피자가 먹고 싶었다.그러나 오빠는 치킨을 먹고 싶다고 했다. 그래서 결국 치킨을 먹었다."

 ]
generate_multiple_questions(model, tokenizer, example_contexts)

example_contexts = [
    # 역사 텍스트
    "고려 시대는 918년에 왕건이 세운 왕조로, 한반도 역사에서 중요한 역할을 했다. 이 시기에는 불교가 국교로 자리 잡으며 문화와 예술이 크게 발전했다. 특히 금속활자 인쇄술이 발달하여 책과 문서의 보급이 확대되었으며, 서민들 사이에서도 교육이 점차 확산되었다. 또한, 몽골 침략과 같은 외부의 위협에도 불구하고 강력한 중앙 집권 체제를 유지하며 사회 질서를 안정시켰다. 하지만 점차 왕권이 약해지면서 내부 권력 다툼이 심화되었고, 결국 조선 왕조가 등장하게 되는 계기가 되었다.",

    # 일상 텍스트
    "오늘 아침에 일어나서 운동을 하고, 커피 한 잔을 마셨다. 날씨가 좋아서 창문을 활짝 열고 신선한 공기를 들이마셨지. 출근길에는 버스가 조금 늦어서 약간 지각할 뻔했지만 다행히 무사히 회사에 도착했다. 점심에는 동료들과 새로운 식당에 가서 맛있는 음식을 먹었고, 오후에는 중요한 회의를 준비하느라 바빴다. 퇴근 후에는 가까운 친구와 전화 통화를 하며 하루 일과를 이야기했다.",

    # 뉴스 텍스트
    "최근 국제 사회에서는 환경 보호와 지속 가능한 발전에 대한 관심이 크게 증가하고 있다. 여러 국가들이 탄소 중립을 목표로 하는 정책을 발표하며, 재생 에너지 사용 확대와 플라스틱 사용 감소에 힘쓰고 있다. 또한, 국제기구들은 기후 변화 대응을 위한 협력 방안을 모색하며 공동의 목표 달성을 위해 노력하고 있다. 이러한 움직임은 글로벌 경제와 산업 구조에도 큰 변화를 불러일으키고 있으며, 시민들의 인식 변화도 함께 이루어지고 있다."
]
generate_one_question_per_context(model, tokenizer, example_contexts)

example_contexts = [
    "서울시는 이날 오후 4시30분부터 강우 상황과 관련해 비상근무 1단계에 돌입해 시 공무원 355명과 25개 자치구 3110명이 폭우에 대비하고 있다고 밝혔다. 17일 새벽까지 50~150㎜ 가량의 비가 쏟아질 것으로 예상된다."
]
generate_one_question_per_context(model, tokenizer, example_contexts)

example_contexts = [
    "1950년대 이후 반도체 기술의 발전은 현대 사회의 산업 구조를 근본적으로 변화시켰다. 1947년 트랜지스터의 발명으로 시작된 반도체 혁신은 집적회로(IC)를 거치며 컴퓨터, 스마트폰, 자동차, 가전제품 등 다양한 분야에 핵심적인 역할을 담당하게 되었다. 최근에는 인공지능과 빅데이터 분석의 수요 증가로 인해 고성능 반도체의 필요성이 더욱 부각되고 있다. 특히, 5나노 이하 공정의 초미세 반도체는 연산 속도와 에너지 효율을 획기적으로 개선하여 글로벌 IT 기업과 반도체 제조업체 간의 치열한 경쟁의 중심에 있다. 이처럼 반도체 산업은 한 국가의 경제 및 안보 측면에서도 전략적으로 중요한 위치를 차지하고 있으며, 각국 정부는 반도체 기술력 확보를 위해 대규모 연구개발 투자와 정책 지원을 확대하고 있다."

]
generate_one_question_per_context(model, tokenizer, example_contexts)

example_contexts = [
    "인공 지능과 로봇 공학의 발전은 제조업뿐만 아니라 교육·의료·예술 등 다양한 분야에 새로운 지평을 열고 있다. 예를 들어, 최근 개발된 의료 진단 AI는 수만 건의 진단 데이터를 분석해 암 발생 가능성을 빠르고 정확하게 판별할 수 있으며, 일부 병원에서는 이미 실제 환자 진료에 도입되고 있다. 한편, 초등학교와 중학교에서는 코딩과 인공지능 기초가 정규 교육과정에 포함되기 시작했고, 일부 교육기관에서는 로봇을 활용해 장애 학생의 학습을 지원하는 프로그램도 운영 중이다. 예술계의 경우 인공지능이 작곡·그림 등 창작 영역으로 점차 확장되고 있는데, 2023년에는 AI가 작곡한 클래식 곡이 국제 음악 경연대회에서 수상하기도 했다. 하지만 이러한 기술 변화가 고용 구조, 윤리적 문제, 개인정보 보호 등 새로운 과제도 함께 제기한다는 점에서 사회적 논의가 활발히 이루어지고 있다."
]

generate_one_question_per_context(model, tokenizer, example_contexts)

example_contexts = [
    "위키백과는 위키를 이용하여 전 세계 사람들이 함께 만들어가는 웹 기반의 다언어 백과사전입니다. 위키백과는 중립적이고 검증 가능한 자유 콘텐츠 백과사전의 제공을 목적으로 하는 프로젝트로, 누구나 참여하여 문서를 수정하고 발전시킬 수 있습니다."
]

generate_one_question_per_context(model, tokenizer, example_contexts)

example_contexts = [
    "위키백과는 다섯 가지 기본 원칙에 따라 운영됩니다. 모든 문서는 크리에이티브 커먼즈 저작자표시-동일조건변경허락 4.0에 따라 사용할 수 있으며, 복사, 수정과 배포가 자유롭고 상업적 목적의 사용도 가능합니다."
]

generate_one_question_per_context(model, tokenizer, example_contexts)

example_contexts = [
    "자연 언어 처리(自然言語處理)는 인간의 언어 현상을 컴퓨터와 같은 기계를 이용해서 묘사할 수 있도록 연구하고 이를 구현하는 컴퓨터과학, 인공지능의 연구 분야 중 하나다. 자연어를 컴퓨터가 이해하기 위해선 프로그래밍 언어로 처리해야 하는데, 컴퓨터가 자연어를 인식 또는 생성할 수 있도록 하는 것을 말한다."
]

generate_one_question_per_context(model, tokenizer, example_contexts)

