# KoBART Question Generation

ì´ ì €ì¥ì†ŒëŠ” **KoBART ëª¨ë¸**ì„ í™œìš©í•˜ì—¬ í•œêµ­ì–´ ë¬¸ë§¥(Context)ìœ¼ë¡œë¶€í„° **ì§ˆë¬¸ì„ ìë™ ìƒì„±**í•˜ëŠ” ì˜ˆì œ ë…¸íŠ¸ë¶ì„ í¬í•¨í•©ë‹ˆë‹¤.  
ì—°êµ¬, êµìœ¡, ë°ì´í„°ì…‹ ìƒì„± ë“± ë‹¤ì–‘í•œ ëª©ì ì—ì„œ í™œìš©í•  ìˆ˜ ìˆìŠµë‹ˆë‹¤.

---

## ğŸš€ Features
- KoBART ê¸°ë°˜ ì§ˆë¬¸ ìƒì„± (Hugging Face Transformers í™œìš©)
- ì—¬ëŸ¬ ê°œì˜ ì§ˆë¬¸ì„ í•œ ë²ˆì— ìƒì„± (`generate_multiple_questions` í•¨ìˆ˜)
- Beam Search, ë°˜ë³µ ë°©ì§€ ì˜µì…˜ ë“± ë‹¤ì–‘í•œ íŒŒë¼ë¯¸í„° ì œê³µ
- ê°„ë‹¨í•œ ì˜ˆì‹œ Contextë¡œ ê²°ê³¼ í™•ì¸

---

## ğŸ“‚ Repository Structure
```
.
â”œâ”€â”€ kobart_question_generation_fixed.ipynb
â””â”€â”€ README.md
```
> `kobart_question_generation_fixed.ipynb` ëŠ” `metadata.widgets` json íŒŒì¼ì„ ìˆ˜ì •í•œ ë²„ì „ì…ë‹ˆë‹¤.

---

## âš™ï¸ Installation

Python 3.8+ í™˜ê²½ì„ ê¶Œì¥í•©ë‹ˆë‹¤.

```bash
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>

pip install torch transformers
```
> âš ï¸ CUDA í™˜ê²½ì´ ìˆë‹¤ë©´ GPU ì‚¬ìš©ì´ ê°€ëŠ¥í•©ë‹ˆë‹¤. (`torch.cuda.is_available()` ìë™ ê°ì§€)

---

## ğŸ–¥ï¸ Usage (Notebook)

Jupyter Notebook ë˜ëŠ” Google Colabì—ì„œ ë…¸íŠ¸ë¶ íŒŒì¼ì„ ì‹¤í–‰í•˜ì„¸ìš”.

### 1) ëª¨ë¸ & í† í¬ë‚˜ì´ì € ë¡œë“œ
```python
import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration

tokenizer = PreTrainedTokenizerFast.from_pretrained("gogamza/kobart-base-v1")
model = BartForConditionalGeneration.from_pretrained("gogamza/kobart-base-v1")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()
```

### 2) ì§ˆë¬¸ ìƒì„± í•¨ìˆ˜ (ìˆ˜ì •ëœ ë²„ì „ ì˜ˆì‹œ)
```python
def generate_multiple_questions(model, tokenizer, contexts, max_length=64, num_questions=3):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    for i, context in enumerate(contexts):
        inputs = tokenizer(
            context,
            max_length=512,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)

        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_length,
            num_beams=10,  # ë¹”ì„œì¹˜ ìˆ˜ (ë” ë†’ìœ¼ë©´ ë‹¤ì–‘ì„±â†‘)
            num_return_sequences=num_questions,  # ì—¬ëŸ¬ ê°œ ì§ˆë¬¸ ìƒì„±
            early_stopping=True,
            no_repeat_ngram_size=2  # ë°˜ë³µ ë°©ì§€
        )

        print(f"=== Context {i+1} ===")
        print(f"í…ìŠ¤íŠ¸: {context}\\n")
        for idx, output in enumerate(outputs):
            question = tokenizer.decode(output, skip_special_tokens=True)
            print(f"ìƒì„±ëœ ì§ˆë¬¸ {idx+1}: {question}")
        print("\\n")
```

### 3) ì˜ˆì‹œ Contextë¡œ ì‹¤í–‰
```python
contexts = [
    "ì§€êµ¬ëŠ” íƒœì–‘ ì£¼ìœ„ë¥¼ ê³µì „í•˜ë©´ì„œ ì‚¬ê³„ì ˆì˜ ë³€í™”ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤. ì´ ê³¼ì •ì—ì„œ ì§€êµ¬ì˜ ìì „ì¶•ì´ ê¸°ìš¸ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— ê° ì§€ì—­ì€ ê³„ì ˆë§ˆë‹¤ ë‹¤ë¥¸ ê¸°ì˜¨ê³¼ ê¸°í›„ë¥¼ ê²½í—˜í•˜ê²Œ ëœë‹¤.",
    "ì‚°ì—… í˜ëª…ì€ 18ì„¸ê¸° í›„ë°˜ ì˜êµ­ì—ì„œ ì‹œì‘ë˜ì–´ ì „ ì„¸ê³„ë¡œ í™•ì‚°ë˜ì—ˆë‹¤. ì¦ê¸°ê¸°ê´€ê³¼ ê¸°ê³„í™”ëœ ìƒì‚° ë°©ì‹ì€ ì‚¬íšŒ êµ¬ì¡°ì™€ ê²½ì œ ì²´ì œë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ ë°”ê¾¸ì–´ ë†“ì•˜ë‹¤."
]

generate_multiple_questions(model, tokenizer, contexts, num_questions=3)
```

---

## ğŸ“ Example Output
ì…ë ¥ Context:
```
ë‹¬ì˜ ì¤‘ë ¥ì€ ì§€êµ¬ì˜ ë°”ë‹·ë¬¼ì— ì˜í–¥ì„ ì£¼ì–´ ë°€ë¬¼ê³¼ ì°ë¬¼ì´ ê·œì¹™ì ìœ¼ë¡œ ë°œìƒí•œë‹¤.
```

ìƒì„±ëœ ì§ˆë¬¸ ì˜ˆì‹œ:
```
1. ë‹¬ì˜ ì¤‘ë ¥ì€ ì§€êµ¬ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?
2. ë°€ë¬¼ê³¼ ì°ë¬¼ì´ ë°œìƒí•˜ëŠ” ì›ì¸ì€ ë¬´ì—‡ì¸ê°€?
3. ì¡°ì„ í˜„ìƒì€ ì–´ë–»ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ”ê°€?
```

---

## âš ï¸ Notes
- `num_beams`ê°€ ë†’ì„ìˆ˜ë¡ ë‹¤ì–‘ì„±ì´ ì¦ê°€í•˜ì§€ë§Œ ì†ë„ê°€ ëŠë ¤ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.(íŠ¹íˆ ì½”ë© í™˜ê²½ì—ì„œ ë§¤ìš° ëŠë¦¬ê²Œ ì‘ë™í•©ë‹ˆë‹¤...)
- ì§ˆë¬¸ í’ˆì§ˆì€ Contextì˜ ê¸¸ì´Â·ë¬¸ì¥ êµ¬ì¡°Â·ë„ë©”ì¸ì— ë”°ë¼ ë‹¬ë¼ì§ˆ ìˆ˜ ìˆìŠµë‹ˆë‹¤.
- ê´„í˜¸ ì˜¤ë¥˜ê°€ ì¬ë°œí•˜ì§€ ì•Šë„ë¡ `model.generate(` ë¸”ë¡ì˜ ì¸ì/ê´„í˜¸ ì§ì„ ì£¼ì„ê³¼ ë¶„ë¦¬í•´ ì‘ì„±í•˜ì„¸ìš”.

---

## ğŸ¤ Contributing
ì´ìŠˆ/PR í™˜ì˜í•©ë‹ˆë‹¤. ë²„ê·¸ ë¦¬í¬íŠ¸ ì‹œ ì¬í˜„ ê°€ëŠ¥í•œ ì½”ë“œ ì¡°ê°ê³¼ í™˜ê²½ ì •ë³´ë¥¼ í¬í•¨í•´ ì£¼ì„¸ìš”.

---

## ğŸ“œ License
MIT License
