# KoBART Question Generation

This repository contains an example notebook that demonstrates how to **automatically generate questions** from Korean contexts using the **KoBART model**.  
It can be applied to research, education, dataset creation, and more.

---

## ğŸš€ Features
- Question generation based on KoBART (using Hugging Face Transformers)
- Generate multiple questions at once (`generate_multiple_questions` function)
- Supports parameters such as Beam Search and no-repeat n-gram
- Test results with simple example contexts
- âš ï¸ Currently works only with **Korean input**

---

## ğŸ“‚ Repository Structure
```
.
â”œâ”€â”€ kobart_question_generation_fixed.ipynb
â””â”€â”€ README_EN.md
```
> `kobart_question_generation_fixed.ipynb` is a cleaned version with `metadata.widgets` JSON fixed.

---

## âš™ï¸ Installation

Python 3.8+ is recommended.

```bash
git clone https://github.com/<your-username>/<repo-name>.git
cd <repo-name>

pip install torch transformers
```
> âš ï¸ If CUDA is available, GPU can be used (automatically detected by `torch.cuda.is_available()`).

---

## ğŸ–¥ï¸ Usage (Notebook)

Run the notebook in Jupyter Notebook or Google Colab.

### 1) Load Model & Tokenizer
```python
import torch
from transformers import PreTrainedTokenizerFast, BartForConditionalGeneration

tokenizer = PreTrainedTokenizerFast.from_pretrained("gogamza/kobart-base-v1")
model = BartForConditionalGeneration.from_pretrained("gogamza/kobart-base-v1")

device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
model.to(device)
model.eval()
```

### 2) Question Generation Function
```python
def generate_multiple_questions(model, tokenizer, contexts, max_length=64, num_questions=3):
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model.to(device)
    model.eval()

    for i, context in enumerate(contexts):
        inputs = tokenizer(
            context,
            max_length=512,
            padding="max_length",
            truncation=True,
            return_tensors="pt"
        )
        input_ids = inputs["input_ids"].to(device)
        attention_mask = inputs["attention_mask"].to(device)

        outputs = model.generate(
            input_ids=input_ids,
            attention_mask=attention_mask,
            max_length=max_length,
            num_beams=10,  # number of beams (higher â†’ more diverse)
            num_return_sequences=num_questions,  # generate multiple questions
            early_stopping=True,
            no_repeat_ngram_size=2  # prevent repetition
        )

        print(f"=== Context {i+1} ===")
        print(f"í…ìŠ¤íŠ¸: {context}\n")
        for idx, output in enumerate(outputs):
            question = tokenizer.decode(output, skip_special_tokens=True)
            print(f"ìƒì„±ëœ ì§ˆë¬¸ {idx+1}: {question}")
        print("\n")
```

### 3) Run with Example Contexts
```python
contexts = [
    "ì§€êµ¬ëŠ” íƒœì–‘ ì£¼ìœ„ë¥¼ ê³µì „í•˜ë©´ì„œ ì‚¬ê³„ì ˆì˜ ë³€í™”ë¥¼ ë§Œë“¤ì–´ë‚¸ë‹¤. ì´ ê³¼ì •ì—ì„œ ì§€êµ¬ì˜ ìì „ì¶•ì´ ê¸°ìš¸ì–´ì ¸ ìˆê¸° ë•Œë¬¸ì— ê° ì§€ì—­ì€ ê³„ì ˆë§ˆë‹¤ ë‹¤ë¥¸ ê¸°ì˜¨ê³¼ ê¸°í›„ë¥¼ ê²½í—˜í•˜ê²Œ ëœë‹¤.",
    "ì‚°ì—… í˜ëª…ì€ 18ì„¸ê¸° í›„ë°˜ ì˜êµ­ì—ì„œ ì‹œì‘ë˜ì–´ ì „ ì„¸ê³„ë¡œ í™•ì‚°ë˜ì—ˆë‹¤. ì¦ê¸°ê¸°ê´€ê³¼ ê¸°ê³„í™”ëœ ìƒì‚° ë°©ì‹ì€ ì‚¬íšŒ êµ¬ì¡°ì™€ ê²½ì œ ì²´ì œë¥¼ ê·¼ë³¸ì ìœ¼ë¡œ ë°”ê¾¸ì–´ ë†“ì•˜ë‹¤."
]

generate_multiple_questions(model, tokenizer, contexts, num_questions=3)
```

---

## ğŸ“ Example Output
Input Context:
```
ë‹¬ì˜ ì¤‘ë ¥ì€ ì§€êµ¬ì˜ ë°”ë‹·ë¬¼ì— ì˜í–¥ì„ ì£¼ì–´ ë°€ë¬¼ê³¼ ì°ë¬¼ì´ ê·œì¹™ì ìœ¼ë¡œ ë°œìƒí•œë‹¤.
```

Generated Questions (in Korean):
```
1. ë‹¬ì˜ ì¤‘ë ¥ì€ ì§€êµ¬ì— ì–´ë–¤ ì˜í–¥ì„ ë¯¸ì¹˜ëŠ”ê°€?
2. ë°€ë¬¼ê³¼ ì°ë¬¼ì´ ë°œìƒí•˜ëŠ” ì›ì¸ì€ ë¬´ì—‡ì¸ê°€?
3. ì¡°ì„ í˜„ìƒì€ ì–´ë–»ê²Œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ”ê°€?
```

---

## âš ï¸ Notes
- A higher `num_beams` increases diversity but can slow down generation (especially in Colab).
- Question quality may vary depending on context length, sentence structure, and domain.
- To avoid syntax errors, ensure parentheses and comments in the `model.generate(` block are properly separated.
- This model is trained on Korean and therefore works **only with Korean input**.

---

## ğŸ¤ Contributing
Contributions and PRs are welcome. For bug reports, please include reproducible code snippets and environment details.

---

## ğŸ“œ License
MIT License
